{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#re library being used \n",
    "# code snippet that will parse an Apache access log line into a bunch of fields \n",
    "format_pat= re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"\n",
    "    r\"(?P<identity>\\S*)\\s\"\n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "#the above fields are used by browser to view the page, together they make a regular expression. \n",
    "#using this regular expression by applying to each line of our access log , the information bits are grouped together into the different fields.\n",
    "#above is a very powerful language for doing pattern matching on a large string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzing a real log file- a real http access log from Apache \n",
    "logPath = \"access_log.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\91839\\MLCourse\\TopPages_Practice.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/91839/MLCourse/TopPages_Practice.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m access \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroupdict()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/91839/MLCourse/TopPages_Practice.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m request \u001b[39m=\u001b[39m access[\u001b[39m'\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/91839/MLCourse/TopPages_Practice.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m (action, URL, protocol) \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39msplit()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/91839/MLCourse/TopPages_Practice.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m URL \u001b[39min\u001b[39;00m URLCounts:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/91839/MLCourse/TopPages_Practice.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     URLCounts[URL] \u001b[39m=\u001b[39m URLCounts[URL] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "#the script below counts up each URL encountered that was requested and no. of times it was requested\n",
    "#the idea is to sort the list containing URL and its count and then get the results.\n",
    "URLCounts = {} #creating a dictionary\n",
    "\n",
    "with open(logPath, \"r\") as f:\n",
    "#to open a file in reading mode in python so as to extract contents and store the file in an object\n",
    "    for line in (l.rstrip() for l in f):\n",
    "    #extracting each line from the f object containing text file \n",
    "        match= format_pat.match(line)\n",
    "        #applying our regular expression on each line of file \n",
    "        if match:\n",
    "        #if any line matches the format of our regular expression i.e contains the fields \n",
    "            access = match.groupdict()\n",
    "\n",
    "            request = access['request']\n",
    "            #we are accessing the request field out of it i.e actual http request (page being requested out of browser)\n",
    "            (action, URL, protocol) = request.split() #splitting the request into the fields of \"action\", \"URL\" and \"protocol\" \n",
    "            if URL in URLCounts: #checking if the URL of the current weblog already exists in our dictionary \n",
    "                URLCounts[URL] = URLCounts[URL] + 1 #the page visit count is increased\n",
    "            else:\n",
    "                URLCounts[URL] = 1 #the URL added as index in dictionary and visit count initiated as 1\n",
    "    #the URL extracted from everyline of the weblog file with the above operation being done\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "#the URLCounts dict is stored in results object and then sorted in reverse manner \n",
    "for result in results[:20]: #iterating through the first 20 elememnts of results \n",
    "    print(result + \": \" + str(URLCounts[result]))\n",
    "\n",
    "#the error we experience due to above code snippet is \"we need more than one value to unpack\"\n",
    "#it means some request fields are not having all three \"action\", \"URL\" and \"protocol\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_\\\\xb0ZP\\\\x07tR\\\\xe5']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "URLCounts = {}\n",
    "#creating URLCounts dictionary to hold the URL in weblogs\n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match= format_pat.match(line)\n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            request = access['request']\n",
    "            fields = request.split()\n",
    "            #splitting the request line in fields as per the parameters present \n",
    "            if (len(fields) != 3):\n",
    "            #the length of the fields of request line is not equal to 3 then printing those fields \n",
    "                print(fields)\n",
    "            #we get empty fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/xmlrpc.php: 68494\n",
      "/wp-login.php: 1923\n",
      "/: 440\n",
      "/blog/: 138\n",
      "/robots.txt: 123\n",
      "/sitemap_index.xml: 118\n",
      "/post-sitemap.xml: 118\n",
      "/page-sitemap.xml: 117\n",
      "/category-sitemap.xml: 117\n",
      "/orlando-headlines/: 95\n",
      "/san-jose-headlines/: 85\n",
      "http://51.254.206.142/httptest.php: 81\n",
      "/comics-2/: 76\n",
      "/travel/: 74\n",
      "/entertainment/: 72\n",
      "/business/: 70\n",
      "/national/: 70\n",
      "/national-headlines/: 70\n",
      "/world/: 70\n",
      "/weather/: 70\n"
     ]
    }
   ],
   "source": [
    "#code snippet to check for garbage values\n",
    "URLCounts = {}\n",
    "\n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match= format_pat.match(line)\n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            request = access['request']\n",
    "            fields = request.split()\n",
    "            if (len(fields) == 3):#taking entry of those fields only whose no. of fields = 3\n",
    "                URL = fields[1]\n",
    "                if URL in URLCounts:\n",
    "                    URLCounts[URL] = URLCounts[URL] + 1\n",
    "                else:\n",
    "                    URLCounts[URL] = 1\n",
    "\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "for result in results[:20]:\n",
    "    #print(result + \": \" + str(URLCounts[result]))\n",
    "\n",
    "#we will get couple of PHP files these are pearl scripts,XML files and so on \n",
    "#not a useful result \n",
    "#the \"/xmlrpc.php: 68494\" denotes the site was under malicious attack and this perl script was being used to guess password. \n",
    "#the \"/wp-login.php: 1923\" was the login script for malicious log in trials\n",
    "#The attacks attempted to execute stuff as well which is logged on weblog\n",
    "# result - wanted : GET requests; unwanted : perl scripts, execution logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 434\n",
      "/blog/: 138\n",
      "/robots.txt: 123\n",
      "/sitemap_index.xml: 118\n",
      "/post-sitemap.xml: 118\n",
      "/page-sitemap.xml: 117\n",
      "/category-sitemap.xml: 117\n",
      "/orlando-headlines/: 95\n",
      "/san-jose-headlines/: 85\n",
      "http://51.254.206.142/httptest.php: 81\n",
      "/comics-2/: 76\n",
      "/travel/: 74\n",
      "/entertainment/: 72\n",
      "/business/: 70\n",
      "/national/: 70\n",
      "/national-headlines/: 70\n",
      "/world/: 70\n",
      "/weather/: 70\n",
      "/about/: 69\n",
      "/defense-sticking-head-sand/: 69\n"
     ]
    }
   ],
   "source": [
    "#code snippet to process only GET requests\n",
    "URLCounts = {}\n",
    "\n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match= format_pat.match(line) #boolean variable \n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            request = access['request']\n",
    "            fields = request.split()\n",
    "            if (len(fields) == 3):#checking the length of the fields to filter out empty or incomplete requests \n",
    "                (action, URL, protocol) = fields\n",
    "                if (action == 'GET'):#extracting only GET requests for webpages by assigning the action parameter as GET \n",
    "                    if URL in URLCounts:\n",
    "                        URLCounts[URL] = URLCounts[URL] + 1\n",
    "                    else:\n",
    "                        URLCounts[URL] = 1\n",
    "\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))\n",
    "#the blog is getting more hits than the news page on the news website which is abnormal \n",
    "#Problem - many blog requests do not have user agent in them ; Cause - malicious traffic or scraper \n",
    "#humans with real browser - user agent : Mozilla or Firefox or Explorer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4221330639.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 23\u001b[1;36m\u001b[0m\n\u001b[1;33m    # web crawlers and bot traffic doesnt count for analysis as it required the traffic by humans.(they are automated scripts)\u001b[0m\n\u001b[1;37m                                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "#code snippet to display different user agent and sort them\n",
    "UserAgents = {}\n",
    "#creating useragent dictionary \n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match= format_pat.match(line)\n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent'] #accessing user_agent field from the access \n",
    "            if agent in UserAgents:\n",
    "                UserAgents[agent] = UserAgents[agent] + 1\n",
    "            else:\n",
    "                UserAgents[agent] = 1\n",
    "\n",
    "results = sorted(UserAgents, key=lambda i: int(UserAgents[i]), reverse=True)\n",
    "\n",
    "for result in results:\n",
    "    #print(result + \": \" + str(UserAgents[result]))\n",
    "\n",
    "#in result we see \"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0): 68484\" - which is a malicious scraper pretending to be a legitimitate browser\n",
    "# the dash \"-\" user agent : dont know the source but definitely not an actual browser \n",
    "# the useragents dict is polluted by web crawlers i.e search engine - Baidu (SE in China), Yandex - (SE in Russia), Google bot and Bing bot. These web crawlers or spiders mine the website for search engine purposes. \n",
    "# web crawlers and bot traffic doesnt count for analysis as it required the traffic by humans.(they are automated scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 77\n",
      "/orlando-headlines/: 36\n",
      "/?page_id=34248: 28\n",
      "/wp-content/cache/minify/000000/M9AvyUjVzUstLy7PLErVz8lMKkosqtTPKtYvTi7KLCgpBgA.js: 27\n",
      "/wp-content/cache/minify/000000/M9bPKixNLarUy00szs8D0Zl5AA.js: 27\n",
      "/wp-content/cache/minify/000000/lY7dDoIwDIVfiG0KxkfxfnbdKO4HuxICTy-it8Zw15PzfSftzPCckJem-x4qUWArqBPl5mygZLEgyhdOaoxToGyGaiALiOfUnIz0qDLOdSZGE-nOlpc3kopDzrSyavVVt_veb5qSDVhjsQ6dHh_B_eE_z2pYIGJ7iBWKeEio_eT9UQe4xHhDll27mGRryVu_pRc.js: 27\n",
      "/wp-content/cache/minify/000000/fY45DoAwDAQ_FMvkRQgFA5ZyWLajiN9zNHR0O83MRkyt-pIctqYFJPedKyYzfHg2PzOFiENAzaD07AxcpKmTolORvDjZt8KEfhBUGjZYCf8Fb0fvA1TXCw.css: 25\n",
      "/?author=1: 21\n",
      "/wp-content/cache/minify/000000/hcrRCYAwDAXAhXyEjiQ1YKAh4SVSx3cE7_uG7ASr4M9qg3kGWyk1adklK84LHtRj_My6Y0Pfqcz-AA.js: 20\n",
      "/wp-content/uploads/2014/11/nhn1.png: 19\n",
      "/wp-includes/js/wp-emoji-release.min.js?ver=4.3.1: 17\n",
      "/wp-content/cache/minify/000000/BcGBCQAgCATAiUSaKYSERPk3avzuht4SkBJnt4tHJdqgnPBqKldesTcN1R8.js: 17\n",
      "/wp-login.php: 16\n",
      "/comics-2/: 12\n",
      "/world/: 12\n",
      "/favicon.ico: 10\n",
      "/wp-content/uploads/2014/11/violentcrime.jpg: 6\n",
      "/robots.txt: 6\n",
      "/wp-content/uploads/2014/11/garfield.jpg: 6\n",
      "/wp-content/uploads/2014/11/babyblues.jpg: 6\n"
     ]
    }
   ],
   "source": [
    "#Problem - Identifying spiders or robots user agents from user string alone\n",
    "#code snippet to identify the spiders or robots or web crawlers \n",
    "URLCounts = {}\n",
    "\n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match= format_pat.match(line)\n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']\n",
    "            if (not('bot' in agent or 'spider' in agent or \n",
    "                    'Bot' in agent or 'Spider' in agent or\n",
    "                    'W3 Total Cache' in agent or agent =='-')):\n",
    "            #filtering out spiders and \"-\" user agent by filtering out the user agents containing word bot or spider or 'W3 Total Cache' anything in caching plugin \n",
    "                request = access['request']\n",
    "                fields = request.split()\n",
    "                if (len(fields) == 3):\n",
    "                    (action, URL, protocol) = fields\n",
    "                    if (action == 'GET'):\n",
    "                        if URL in URLCounts:\n",
    "                            URLCounts[URL] = URLCounts[URL] + 1\n",
    "                        else:\n",
    "                            URLCounts[URL] = 1\n",
    "\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))\n",
    "#the results show that apart from webpages there are some scripts and bunch of css files \n",
    "#as we only require webpages and given: \"the pages in site end with a slash in url\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 77\n",
      "/orlando-headlines/: 36\n",
      "/comics-2/: 12\n",
      "/world/: 12\n",
      "/weather/: 4\n",
      "/australia/: 4\n",
      "/about/: 4\n",
      "/national-headlines/: 3\n",
      "/feed/: 2\n",
      "/sample-page/feed/: 2\n",
      "/science/: 2\n",
      "/technology/: 2\n",
      "/entertainment/: 1\n",
      "/san-jose-headlines/: 1\n",
      "/business/: 1\n",
      "/travel/feed/: 1\n"
     ]
    }
   ],
   "source": [
    "#code snippet that refines the search and removes any URL entry that doesnt end with a slash\n",
    "URLCounts = {}\n",
    "\n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match= format_pat.match(line)\n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']\n",
    "            if (not('bot' in agent or 'spider' in agent or \n",
    "                    'Bot' in agent or 'Spider' in agent or\n",
    "                    'W3 Total Cache' in agent or agent =='-')):\n",
    "                request = access['request']\n",
    "                fields = request.split()\n",
    "                if (len(fields) == 3):\n",
    "                    (action, URL, protocol) = fields\n",
    "                    if (URL.endswith(\"/\")):\n",
    "                        if (action == 'GET'):\n",
    "                            if URL in URLCounts:\n",
    "                                URLCounts[URL] = URLCounts[URL] + 1\n",
    "                            else:\n",
    "                                URLCounts[URL] = 1\n",
    "\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))\n",
    "#the results show that feed pages but in actuality many are because of robots trying to to get RSS data from website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 77\n",
      "/orlando-headlines/: 36\n",
      "/comics-2/: 12\n",
      "/world/: 12\n",
      "/weather/: 4\n",
      "/australia/: 4\n",
      "/about/: 4\n",
      "/national-headlines/: 3\n",
      "/science/: 2\n",
      "/technology/: 2\n",
      "/entertainment/: 1\n",
      "/san-jose-headlines/: 1\n",
      "/business/: 1\n"
     ]
    }
   ],
   "source": [
    "URLCounts = {}\n",
    "#code snippet to filter out the feed pages from frequently visited pages of the site \n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match= format_pat.match(line)\n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']\n",
    "            if (not('bot' in agent or 'spider' in agent or \n",
    "                    'Bot' in agent or 'Spider' in agent or\n",
    "                    'W3 Total Cache' in agent or agent =='-')):\n",
    "                request = access['request']\n",
    "                fields = request.split()\n",
    "                if (len(fields) == 3):\n",
    "                    (action, URL, protocol) = fields\n",
    "                    if (URL.endswith(\"/\")):\n",
    "                        if (action == 'GET'):\n",
    "                            if (not('feed' in URL)):\n",
    "                                if URL in URLCounts:\n",
    "                                    URLCounts[URL] = URLCounts[URL] + 1\n",
    "                                else:\n",
    "                                    URLCounts[URL] = 1\n",
    "\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36: 4\n",
      "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0): 1\n"
     ]
    }
   ],
   "source": [
    "FeedAgents = {}\n",
    "#code snippet to visualize the user agents accessing the feed pages \n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match= format_pat.match(line)\n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']\n",
    "            if (not('bot' in agent or 'spider' in agent or \n",
    "                    'Bot' in agent or 'Spider' in agent or\n",
    "                    'W3 Total Cache' in agent or agent =='-')):\n",
    "                request = access['request']\n",
    "                fields = request.split()\n",
    "                if (len(fields) == 3):\n",
    "                    (action, URL, protocol) = fields\n",
    "                    if (URL.endswith(\"/\")):\n",
    "                        if (action == 'GET'):\n",
    "                            if ('feed' in URL):\n",
    "                                if agent in FeedAgents:\n",
    "                                    FeedAgents[agent]+=1\n",
    "                                else:\n",
    "                                    FeedAgents[agent]=1\n",
    "\n",
    "\n",
    "results = sorted(FeedAgents, key=lambda i: int(FeedAgents[i]), reverse=True)\n",
    "\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(FeedAgents[result]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
